{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c18c434",
   "metadata": {},
   "source": [
    "# SMS Spam Collection using NLP\n",
    "\n",
    "Link to Data Set : https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "**Requirements: You will need to have NLTK installed, along with downloading the corpus for stopwords. To download everything with a conda installation, run the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec13fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN THIS CELL IF YOU NEED \n",
    "# TO DOWNLOAD NLTK AND HAVE CONDA\n",
    "# WATCH THE VIDEO FOR FULL INSTRUCTIONS ON THIS STEP\n",
    "\n",
    "# Uncomment the code below and run:\n",
    "\n",
    "# !conda install nltk #This installs nltk\n",
    "# import nltk # Imports the library\n",
    "# nltk.download() #Download the necessary datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a6239",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import nltk, pandas, numpy, matplotlib,and seaborn. Then set %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74821ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd0572",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "We'll be using a dataset from the UCI datasets! This dataset is already located in the folder for this project.\n",
    "\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
    "\n",
    "We will be using this data to build a spam detection filter.\n",
    "\n",
    "Let's go ahead and use rstrip() plus a list comprehension to get a list of all the lines of text messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93549167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "messages = [line.rstrip() for line in open('SMSSpamCollection')]\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832296b4",
   "metadata": {},
   "source": [
    "A collection of texts is also sometimes called \"corpus\". Let's print the first nine messages and number them using **enumerate**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c387fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message_no, message in enumerate(messages[:9]):\n",
    "    print(message_no, message)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9c67e",
   "metadata": {},
   "source": [
    "## Splitting Text\n",
    "\n",
    "This is a [TSV] (\"tab separated values\") file, which can be identified by the spacing, and the first column of the file contains a label indicating whether the supplied message is a regular message (often referred to as \"ham\") or \"spam.\" The message itself is in the second column. (Note that our numbers are only from the **enumerate** call; they are not a part of the file.)\n",
    "\n",
    "We'll build a machine learning model to automatically distinguish between spam and ham using these instances of labelled ham. We will then be able to categorise random unlabeled messages as spam or ham using a trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85955a00",
   "metadata": {},
   "source": [
    "**We can simply use pandas instead of manually parsing TSV in Python**\n",
    "\n",
    "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df270391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = pd.read_csv('SMSSpamCollection', sep='\\t',\n",
    "                           names=[\"label\", \"message\"])\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe94124",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's check out some of the stats with some plots and the built-in methods in pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6927c1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539e47d",
   "metadata": {},
   "source": [
    "Let's use **groupby** to use describe by label, this way we can begin to think about the features that separate ham and spam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23b4017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aef044",
   "metadata": {},
   "source": [
    "We want to begin considering the features we'll be employing as we carry out our analysis. This is consistent with the feature engineering philosophy in general. Your capacity to develop new characteristics from the data depends on how well you understand its domain. In general, feature engineering plays a significant role in spam detection.\n",
    "\n",
    "Let's make a new column to detect how long the text messages are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac0cec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg['length'] = msg['message'].apply(len)\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e850b",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fff3130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "msg['length'].plot(bins=50, kind='hist') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c25da",
   "metadata": {},
   "source": [
    "Change the bin size around! It appears that considering text length is an useful aspect to consider! Let's try to explain why the x-axis extends all the way to 1000 or so; there must be a really lengthy message here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ebaa171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a43b0e",
   "metadata": {},
   "source": [
    "910 characters, let's use masking to find this message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b01026c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg[msg['length'] == 910]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bf592",
   "metadata": {},
   "source": [
    "It appears that we have a Romeo sending SMS! Returning to the question of whether message length may be used to differentiate between ham and spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e000662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<AxesSubplot:title={'center':'ham'}>,\n",
       "       <AxesSubplot:title={'center':'spam'}>], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "msg.hist(column='length', by='label', bins=50,figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149d803",
   "metadata": {},
   "source": [
    "Very interesting! Through just basic EDA we've been able to discover a trend that spam messages tend to have more characters. \n",
    "\n",
    "Let's start processing the data so that we can eventually use SciKit Learn with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e58fd1",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb4ca1",
   "metadata": {},
   "source": [
    "The fact that all of our data is in text format is our major problem (strings). The classification task will be carried out by the classification algorithms that we have learned about so far using some kind of numerical feature vector. A corpus can be converted to a vector format using a variety of techniques. The [bag-of-words] technique, where each distinct word in a text is represented by one integer, is the simplest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac9aef",
   "metadata": {},
   "source": [
    "The raw messages (sequence of characters) will be converted into vectors in this phase (sequences of numbers).\n",
    "\n",
    "Let's start by creating a function that breaks down a message into its component words and returns a list. We'll also get rid of words like \"the\" and \"a,\" etc. We will make use of the NLTK library to accomplish this. It is essentially the default Python text processing module and includes a tonne of helpful features. Here, we'll merely employ a few of the fundamental ones.\n",
    "\n",
    "Create a method to handle the text in the message column, and then use pandas' **apply()** function to handle all the text in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce356350",
   "metadata": {},
   "source": [
    "First removing punctuation. We can just take advantage of Python's built-in **string** library to get a quick list of all the possible punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea576b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "messeage = 'Sample message! Notice: it has punctuation.'\n",
    "\n",
    "# Check characters to see if they are in punctuation\n",
    "nopunc = [char for char in messeage if char not in string.punctuation]\n",
    "\n",
    "# Join the characters again to form the string.\n",
    "nopunc = ''.join(nopunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b48fd",
   "metadata": {},
   "source": [
    "Now let's see how to remove stopwords. We can impot a list of english stopwords from NLTK (check the documentation for more languages and info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80dad8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[0:10] # Show some stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45486fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a659ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now just remove any stopwords\n",
    "clean_messeage = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91fd07ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_messeage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24c910",
   "metadata": {},
   "source": [
    "Now let's put both of these together in a function to apply it to our DataFrame later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc89c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a string of text, then performs the following:\n",
    "# 1. Remove all punctuation\n",
    "# 2. Remove all stopwords\n",
    "# 3. Returns a list of the cleaned text\n",
    "    \n",
    "def text_process(messeage):\n",
    "   \n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in messeage if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e75de9",
   "metadata": {},
   "source": [
    "Here is the original DataFrame again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed1c20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acccd6c9",
   "metadata": {},
   "source": [
    "Let's \"tokenize\" these communications right now. Tokenization simply refers to the procedure of turning a list of regular text strings into a collection of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13282732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure its working\n",
    "msg['message'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed96f06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show original dataframe\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d61c8",
   "metadata": {},
   "source": [
    "## Stemming Text\n",
    "\n",
    "Stemming is the process of reducing inflected or derived words to their word stem or root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3822a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38ffec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1339dbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[g, o,  , u, n, t, i, l,  , j, u, r, o, n, g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[o, k,  , l, a, r, ., ., .,  , j, o, k, i, n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[f, r, e, e,  , e, n, t, r, y,  , i, n,  , 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[u,  , d, u, n,  , s, a, y,  , s, o,  , e, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[n, a, h,  , i,  , d, o, n, ', t,  , t, h, i, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                                stem  \n",
       "0  [g, o,  , u, n, t, i, l,  , j, u, r, o, n, g, ...  \n",
       "1  [o, k,  , l, a, r, ., ., .,  , j, o, k, i, n, ...  \n",
       "2  [f, r, e, e,  , e, n, t, r, y,  , i, n,  , 2, ...  \n",
       "3  [u,  , d, u, n,  , s, a, y,  , s, o,  , e, a, ...  \n",
       "4  [n, a, h,  , i,  , d, o, n, ', t,  , t, h, i, ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make new \"stemmed words data\" column\n",
    "msg['stem'] = msg['message'].apply(lambda x: stemming(x))\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f83bda",
   "metadata": {},
   "source": [
    "## Lemmatizing Text\n",
    "\n",
    "Lemmatizing is the process of returning a word to its dictionary form by removing inflectional endings. Although it requires more resources, this text cleaning technique is more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "174703e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8c908e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[g, o,  , u, n, t, i, l,  , j, u, r, o, n, g, ...</td>\n",
       "      <td>[G, o,  , u, n, t, i, l,  , j, u, r, o, n, g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[o, k,  , l, a, r, ., ., .,  , j, o, k, i, n, ...</td>\n",
       "      <td>[O, k,  , l, a, r, ., ., .,  , J, o, k, i, n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[f, r, e, e,  , e, n, t, r, y,  , i, n,  , 2, ...</td>\n",
       "      <td>[F, r, e, e,  , e, n, t, r, y,  , i, n,  , 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[u,  , d, u, n,  , s, a, y,  , s, o,  , e, a, ...</td>\n",
       "      <td>[U,  , d, u, n,  , s, a, y,  , s, o,  , e, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[n, a, h,  , i,  , d, o, n, ', t,  , t, h, i, ...</td>\n",
       "      <td>[N, a, h,  , I,  , d, o, n, ', t,  , t, h, i, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [g, o,  , u, n, t, i, l,  , j, u, r, o, n, g, ...   \n",
       "1  [o, k,  , l, a, r, ., ., .,  , j, o, k, i, n, ...   \n",
       "2  [f, r, e, e,  , e, n, t, r, y,  , i, n,  , 2, ...   \n",
       "3  [u,  , d, u, n,  , s, a, y,  , s, o,  , e, a, ...   \n",
       "4  [n, a, h,  , i,  , d, o, n, ', t,  , t, h, i, ...   \n",
       "\n",
       "                                                 lem  \n",
       "0  [G, o,  , u, n, t, i, l,  , j, u, r, o, n, g, ...  \n",
       "1  [O, k,  , l, a, r, ., ., .,  , J, o, k, i, n, ...  \n",
       "2  [F, r, e, e,  , e, n, t, r, y,  , i, n,  , 2, ...  \n",
       "3  [U,  , d, u, n,  , s, a, y,  , s, o,  , e, a, ...  \n",
       "4  [N, a, h,  , I,  , d, o, n, ', t,  , t, h, i, ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make new \"lemmatized words data\" column\n",
    "msg['lem'] = msg['message'].apply(lambda x: lemmatizing(x))\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2db0a",
   "metadata": {},
   "source": [
    "At the end even though we did the stemming and lemmatization, we didnt actually use it, we just showed that it would be done for this dataset as instructed in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab82ca",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4668737",
   "metadata": {},
   "source": [
    "The messages must now be transformed from lists of tokens, or \"lemmas,\" that are how we now have them into vectors that SciKit Learn's algorithm models can use.\n",
    "\n",
    "We will now translate each message, which was previously represented as a list of tokens (lemmas), into a vector that computer learning models can comprehend.\n",
    "\n",
    "We'll carry out that utilising the bag-of-words paradigm in three steps:\n",
    "\n",
    "1. Determine the frequency of each word in each message (Known as term frequency)\n",
    "\n",
    "2. Weigh the counts so that tokens used frequently are given less weight (inverse document frequency)\n",
    "\n",
    "3. To remove the original text length, normalise the vectors to a unit length (L2 norm)\n",
    "\n",
    "Let's start with the first phase:\n",
    "\n",
    "We can anticipate a lot of zero counts for the word's presence in the document because there are so many messages. SciKit Learn will produce a Sparse Matrix as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa705f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4987819",
   "metadata": {},
   "source": [
    "There are a lot of arguments and parameters that can be passed to the CountVectorizer. In this case we will just specify the **analyzer** to be our own previously defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3d4d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "# Might take awhile...\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(msg['message'])\n",
    "\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4d24e",
   "metadata": {},
   "source": [
    "Let's take one text message and get its bag-of-words counts as a vector, putting to use our new bow_transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cce984c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "message4 = msg['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add3c7c",
   "metadata": {},
   "source": [
    "Let's examine it in vector form now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf7d31a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n",
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9b4e7",
   "metadata": {},
   "source": [
    "This indicates that message number 4 has seven distinct words (after removing common stop words). The rest only appear once, while two of them do so twice. Let's confirm which ones do indeed show twice by checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df893585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UIN\n",
      "schedule\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[4073])\n",
    "print(bow_transformer.get_feature_names()[9570])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e1370",
   "metadata": {},
   "source": [
    "Now we can use **.transform** on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of messages. Let's go ahead and check out how the bag-of-words counts for the entire SMS corpus is a large, sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df721cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(msg['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63bf2fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (5572, 11425)\n",
      "Amount of Non-Zero occurences:  50548\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22b106ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(round(sparsity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fdf8e",
   "metadata": {},
   "source": [
    "After the counting, the term weighting and normalization can be done with [TF-IDF] by using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107d139",
   "metadata": {},
   "source": [
    "### So what is TF-IDF?\n",
    "\n",
    "Information retrieval and text mining frequently employ the TF-IDF weight, which stands for term frequency-inverse document frequency. This weight is a statistical metric used to assess a word's significance to a collection or corpus of documents. The frequency of the term in the corpus offsets the importance, which rises in direct proportion to the frequency of a word in the document. Search engines frequently utilise variations of the tf-idf weighting scheme as their main scoring and ranking mechanism for determining how relevant a document is to a user query.\n",
    "\n",
    "The tf-idf for each query term is added to create one of the most basic ranking functions; many more complex ranking functions are variations of this basic model.\n",
    "\n",
    "The normalised Term Frequency (TF) is calculated as the logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears. The Inverse Document Frequency (IDF) is computed as the logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe7e3d",
   "metadata": {},
   "source": [
    "Let's go ahead and see how we can do this in SciKit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5b43e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4ac6b",
   "metadata": {},
   "source": [
    "We'll go ahead and check what is the IDF (inverse document frequency) of the word `\"u\"` and of word `\"university\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06d1f6fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2800524267409408\n",
      "8.527076498901426\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\n",
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3999ad",
   "metadata": {},
   "source": [
    "To transform the entire bag-of-words corpus into TF-IDF corpus at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4dedc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae69c64",
   "metadata": {},
   "source": [
    "The data can be preprocessed and vectorized in a variety of ways. These actions entail designing a \"pipeline\" and performing feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b63c1",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1da6d2",
   "metadata": {},
   "source": [
    "Our spam/ham classifier can now be trained using messages as vectors. Actually, we can apply practically any classification algorithm today. The Naive Bayes classification algorithm is a great choice for a variety of reasons.\n",
    "\n",
    "Scikit-learn will be used, with the Naive Bayes classifier as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4e7ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf, msg['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de217811",
   "metadata": {},
   "source": [
    "Let's try classifying our single random message and checking how we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2e6a0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ham\n",
      "expected: ham\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detect_model.predict(tfidf4)[0])\n",
    "print('expected:', msg.label[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0dbc5c",
   "metadata": {},
   "source": [
    "**Fantastic! We've created a model that can try to forecast the classification of spam versus ham!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdeb19",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We now want to assess how well our model will perform across the board on the complete dataset. Let's start by gathering all the forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1594414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detect_model.predict(messages_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a019b21",
   "metadata": {},
   "source": [
    "We may use the classification report that is integrated into SciKit Learn and returns [accuracy, recall,] [f1-score], and a column for support (meaning how many cases supported that classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5eb4f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      4825\n",
      "        spam       1.00      0.85      0.92       747\n",
      "\n",
      "    accuracy                           0.98      5572\n",
      "   macro avg       0.99      0.92      0.95      5572\n",
      "weighted avg       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(msg['label'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0ade3",
   "metadata": {},
   "source": [
    "There are many other metrics that might be used to assess model performance. The task at hand and the financial impact of any decisions made using the model will determine which is the most crucial. For instance, the cost of mistaking \"spam\" for \"ham\" is most likely significantly smaller than the cost of mistaking \"ham\" for \"spam.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78310662",
   "metadata": {},
   "source": [
    "The accuracy of the model was assessed using the same training data in the aforementioned \"assessment\". Never truly evaluate using the same dataset that you trained on!\n",
    "\n",
    "We can learn nothing about our model's actual forecasting ability from such an evaluation. Even though we wouldn't be able to categorise any new messages, the accuracy on training data would trivially be 100% if we merely remembered every sample throughout training.\n",
    "\n",
    "The data should be divided into a training set and a test set, with the training set only ever being seen by the model during model fitting and parameter tuning. No use is ever made of the test results. This is our final assessment of the test data's ability to accurately predict outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fb494",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c43d94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457 1115 5572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(msg['message'], msg['label'], test_size=0.2)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc1304",
   "metadata": {},
   "source": [
    "The test size is 20% of the entire dataset (1115 messages out of total 5572), and the training is the rest (4457 out of 5572). Note the default split would have been 30/70."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c9541",
   "metadata": {},
   "source": [
    "## Creating a Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516bcd19",
   "metadata": {},
   "source": [
    "Let's rerun our model and make predictions based on the test set. SciKit Learn's pipeline features will be used by us to store a workflow pipeline. This will enable us to set up all of the data transformations we'll make for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b860c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5924f1",
   "metadata": {},
   "source": [
    "Now we can directly pass message text data and the pipeline will do our pre-processing for us! We can treat it as a model/estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "870dea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x00000172C7167B80>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66783ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd3c58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.97      0.98      1013\n",
      "        spam       0.75      1.00      0.86       102\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.88      0.98      0.92      1115\n",
      "weighted avg       0.98      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4946b",
   "metadata": {},
   "source": [
    "Now we have a classification report for our model on a true testing set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
